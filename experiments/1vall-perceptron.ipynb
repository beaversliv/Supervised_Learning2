{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "We import our code and any frequently used libraries, and set up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "    \n",
    "DATA_PATH = '../data/zipcombo.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.kernels import polynomial_kernel\n",
    "from src.perceptrons import VectorizedOneVsAllKernelPerceptron, OneVsAllKernelPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Let's initially work with a smaller dataset until we sort out inner and outer efficiency issues (i.e. using one-vs-all and making the individual perceptrons more efficient) <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=' ', header=None).drop(columns=[257])\n",
    "df.rename(columns={0: 'label'}, inplace=True)\n",
    "X = df[list(range(1, 257))].values\n",
    "y = df['label'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not currently use subsampling, but we do still need the function for testing purposes\n",
    "\n",
    "def subsample(df, classes, sample_size=100):\n",
    "    # sampling\n",
    "    df_small = pd.DataFrame()\n",
    "    for clazz in classes:\n",
    "        df_clazz = df[df['y'] == clazz]\n",
    "        df_sample = df_clazz.sample(sample_size)\n",
    "        df_small = df_small.append(df_sample)\n",
    "\n",
    "    #shuffle\n",
    "    df_small = df_small.sample(frac=1.)\n",
    "\n",
    "    X_small = df_small.drop(columns='y').values\n",
    "    y_small = df_small['y'].values\n",
    "    \n",
    "    return X_small, y_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for creating confusion error matrix\n",
    "\n",
    "def conf_mat(X, y, model):\n",
    "    cats = 10\n",
    "    con_mat = np.zeros((cats,cats))\n",
    "    x_pred = model.predict_all(X)\n",
    "    for i in range(len(y)):\n",
    "        con_mat[y[i], x_pred[i]] += 1\n",
    "    return con_mat\n",
    "\n",
    "def confusion_error(X, y, model):\n",
    "    cats = 10\n",
    "    con_mat = np.zeros((cats,cats))\n",
    "    x_pred = model.predict_all(X)\n",
    "    for i in range(len(y)):\n",
    "        con_mat[y[i], x_pred[i]] += 1\n",
    "\n",
    "        \n",
    "    # row normalize\n",
    "    for j in range(0,cats):\n",
    "        list_i = list(range(0,cats))\n",
    "        list_i.remove(j)\n",
    "        tot = sum(con_mat[j, :]) - con_mat[j,j]\n",
    "        for col in list_i:\n",
    "            if con_mat[j, col] != 0:\n",
    "                con_mat[j, col] = (con_mat[j, col]/tot)*100\n",
    "        con_mat[j, j] = 0\n",
    "    return con_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def error_score(y, y_pred):\n",
    "    return 1 - accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Results\n",
    "We split our data into 80%/20% train and test. We perform 20 runs for $d = 1, ..., 7$, and report the mean test and training errors with their standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic run for part 1.1\n",
    "\n",
    "def basic_run(X_train, X_test, y_train, y_test, kernel, epochs=2, progress=False):    \n",
    "    #fit model\n",
    "    mkp = VectorizedOneVsAllKernelPerceptron(X_train, y_train, kernel)\n",
    "    mkp.train_for_epochs(epochs, progress=progress)\n",
    "    \n",
    "    #return errors\n",
    "    error_train = error_score(y_train, mkp.predict_all(X_train))\n",
    "    error_test = error_score(y_test, mkp.predict_all(X_test))\n",
    "    \n",
    "    return {'err_train': error_train, 'err_test': error_test, 'model': mkp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:57<00:00, 31.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# perform basic runs\n",
    "iterations = 20\n",
    "ds = list(range(1, 8))\n",
    "err_train = {d: [] for d in ds}\n",
    "err_test = {d: [] for d in ds}\n",
    "\n",
    "for iteration in tqdm(list(range(iterations))):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    for d in ds:\n",
    "        #split data\n",
    "        results = basic_run(X_train, X_test, y_train, y_test, polynomial_kernel(d), 5)\n",
    "        err_train[d].append(results['err_train'])\n",
    "        err_test[d].append(results['err_test'])\n",
    "    \n",
    "err_train_mean = {d: np.mean(errs) for d, errs in err_train.items()}\n",
    "err_test_mean = {d: np.mean(errs) for d, errs in err_test.items()}\n",
    "err_train_std = {d: np.std(errs) for d, errs in err_train.items()}\n",
    "err_test_std = {d: np.std(errs) for d, errs in err_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "      <th>train_std</th>\n",
       "      <th>test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.929739</td>\n",
       "      <td>0.907231</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.010948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993063</td>\n",
       "      <td>0.961962</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998118</td>\n",
       "      <td>0.967554</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.003703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.971720</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.004483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.971371</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999718</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.971102</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_mean  test_mean  train_std  test_std\n",
       "1    0.929739   0.907231   0.008080  0.010948\n",
       "2    0.993063   0.961962   0.002525  0.005137\n",
       "3    0.998118   0.967554   0.000893  0.003703\n",
       "4    0.999106   0.971720   0.000899  0.004483\n",
       "5    0.999597   0.971371   0.000195  0.004287\n",
       "6    0.999718   0.972043   0.000194  0.003896\n",
       "7    0.999751   0.971102   0.000129  0.002379"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display in dataframe\n",
    "df_err = pd.DataFrame([err_train_mean, err_test_mean,\n",
    "                       err_train_std, err_test_std], \n",
    "                       index=['train_mean', 'test_mean', 'train_std', 'test_std'], \n",
    "                       columns=ds).T\n",
    "df_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-validation\n",
    "\n",
    "We split our data into 80%/20% train and test. We then use 5-fold cross validation to find our best $d^*$ parameter for $d^* \\in \\{1, ..., 7\\}$. We then retrain our optimal kernelised perceptron on the full training set, and calculate training and test errors over 20 runs. We report the mean test and training errors for this perceptron, as well as its standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fold_indices(n, k=5):\n",
    "    ixs = np.array(range(n))\n",
    "    np.random.shuffle(ixs)\n",
    "    folds = np.array_split(ixs, k)\n",
    "    fold_ixs = np.zeros(n)\n",
    "    for i in range(k):\n",
    "        fold_ixs[folds[i]] = i\n",
    "    return fold_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate k folds and perform cross-validation on them, returning error per fold.\n",
    "def cross_validation_error(X, y, kernel, epochs=2, k=5):\n",
    "    fold_ixs = make_fold_indices(len(X), k=k)\n",
    "\n",
    "    cv_errs = []\n",
    "    for fold_ix in np.unique(fold_ixs):\n",
    "        X_val = X[fold_ixs == fold_ix]\n",
    "        y_val = y[fold_ixs == fold_ix]\n",
    "        X_train = X[fold_ixs != fold_ix]\n",
    "        y_train = y[fold_ixs != fold_ix]\n",
    "        \n",
    "        #fit model\n",
    "        mkp = VectorizedOneVsAllKernelPerceptron(X_train, y_train, kernel)\n",
    "        mkp.train_for_epochs(epochs=5)\n",
    "        \n",
    "        #record validation fold error\n",
    "        mkp.train_for_epochs(epochs)\n",
    "        cv_errs.append(error_score(y_val, mkp.predict_all(X_val)))\n",
    "        \n",
    "    return np.mean(cv_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross-validation runs\n",
    "\n",
    "iterations = 5\n",
    "ds = list(range(1, 8))\n",
    "errs_cv = {}\n",
    "\n",
    "d_stars = []\n",
    "errs_test = []\n",
    "confusion_matrices = []\n",
    "for iteration in tqdm(list(range(iterations))):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    # perform cross validations\n",
    "    for d in ds:\n",
    "        errs_cv[d] = cross_validation_error(X_train, y_train, polynomial_kernel(d))\n",
    "        \n",
    "    # get best parameter\n",
    "    d_star = min(errs_cv, key=errs_cv.get)\n",
    "    d_stars.append(d_star)\n",
    "    \n",
    "    # get final error\n",
    "    results = basic_run(X_train, X_test, y_train, y_test, polynomial_kernel(d_star))\n",
    "    errs_test.append(results['err_test'])\n",
    "    \n",
    "    # compute confusion matrices too (so as to avoid recomputing in Q3)\n",
    "    confusion_matrices.append(confusion_error(X_test, y_test, results['model']))\n",
    "    \n",
    "# compute results   \n",
    "err_test_mean = np.mean(errs_test)\n",
    "d_star_mean = np.mean(d_stars)\n",
    "err_test_std = np.std(errs_test)\n",
    "d_star_std = np.std(d_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in dataframe\n",
    "df_err = pd.DataFrame([[err_test_mean, err_test_std],\n",
    "                       [d_star_mean, d_star_std]], \n",
    "                       columns=['mean', 'std'], index=['err_test', 'd_star']).T\n",
    "print(\"Answer to 2:\")\n",
    "df_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "err_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "We compute the confusion matrix for the above perceptron. (It's not clear to me here whether Mark means a confusion matrix for the training set or testing set. I think it makes sense to look at both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in dataframe\n",
    "confusion_matrix_array = np.array(confusion_matrices)\n",
    "confus_error_mean = np.mean(confusion_matrix_array, axis=0)\n",
    "confus_error_std = np.std(confusion_matrix_array, axis=0)\n",
    "\n",
    "df_cm_means = pd.DataFrame(confus_error_mean)\n",
    "print(df_cm_means)\n",
    "\n",
    "df_cm_std = pd.DataFrame(confus_error_std)\n",
    "print(df_cm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hardest Predictions\n",
    "\n",
    "We define \"hardest to predict\" as: ... \n",
    "\n",
    "We then print out the five hardest to predict images, and discuss each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat 1 with a gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_list = np.arange(0.5, 1.7, 0.2)\n",
    "df_gaus = basic_res(X, y, 3, width_list, kernel='gaussian')\n",
    "df_gaus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat 2 with a gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_gaussian = cross_val_run(X, y, runs=3, epochs=2, kernel='gaussian')\n",
    "q2_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basic Results: Perform 20 runs for d = 1, . . . , 7 each run should randomly split zipcombo into 80%\n",
    "train and 20% test. Report the mean test and train errors as well as well as standard deviations.\n",
    "Thus your data table, here, will be 2 × 7 with each “cell” containing a mean±std.\n",
    "2. Cross-validation: Perform 20 runs : when using the 80% training data split from within to perform\n",
    "5-fold cross-validation to select the “best” parameter d\n",
    "∗\n",
    "then retrain on full 80% training set using\n",
    "d\n",
    "∗ and then record the test errors on the remaining 20%. Thus you will find 20 d\n",
    "∗ and 20 test errors.\n",
    "Your final result will consist of a mean test error±std and a mean d\n",
    "∗ with std.\n",
    "3. Confusion matrix: Perform 20 runs : when using the 80% training data split that further to perform\n",
    "5-fold cross-validation to select the “best” parameter d\n",
    "∗\n",
    "retrain on the 80% training using d\n",
    "∗ and\n",
    "produce a confusion matrix. Here the goal is to find “confusions” thus if the true label was “7” and\n",
    "“2” was predicted then a “mistake” should recorded for “(7,2)”; the final output will be a 10 × 10\n",
    "matrix where each cell contains a confusion error and its standard deviation. Note the diagonal will\n",
    "be 0.\n",
    "4. Within the dataset relative to your experiments there will be five hardest to predict correctly “data\n",
    "items.” Print out the visualisation of these five digits along with their labels. Is it surprising that\n",
    "these are hard to predict?\n",
    "5. Repeat 1 and 2 (d\n",
    "∗\n",
    "is now c and {1, . . . , 7} is now S) above with a Gaussian kernel\n",
    "K(p, q) = e\n",
    "−ckp−qk\n",
    "2\n",
    ",\n",
    "c the width of the kernel is now a parameter which must be optimised during cross-validation however,\n",
    "you will also need to perform some initial experiments to a decide a reasonable set S of values to crossvalidate c over.\n",
    "6. Choose (research) an alternate method to generalise to k-classes then repeat 1 and 2.\n",
    "7. Choose two more algorithms to compare to the kernel perceptron each of these algorithms will have\n",
    "a parameter vector θ and you will need to determine a cross-validation set Sθ with this information\n",
    "repeat 1 and 2 for your new algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
