{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "We import our code and any frequently used libraries, and set up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/zipcombo.dat'\n",
    "SRC_PATH = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(SRC_PATH))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.MLkNN import MLkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=' ', header=None).drop(columns=[257])\n",
    "df.rename(columns={0: 'label'}, inplace=True)\n",
    "X = df[list(range(1, 257))].values\n",
    "y = df['label'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not currently use subsampling, but we keep the function for testing purposes\n",
    "\n",
    "def subsample(df, classes, sample_size=100):\n",
    "    # sampling\n",
    "    df_small = pd.DataFrame()\n",
    "    for clazz in classes:\n",
    "        df_clazz = df[df['y'] == clazz]\n",
    "        df_sample = df_clazz.sample(sample_size)\n",
    "        df_small = df_small.append(df_sample)\n",
    "\n",
    "    #shuffle\n",
    "    df_small = df_small.sample(frac=1.)\n",
    "\n",
    "    X_small = df_small.drop(columns='y').values\n",
    "    y_small = df_small['y'].values\n",
    "    \n",
    "    return X_small, y_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "df['y'] = y\n",
    "X, y = subsample(df, list(range(10)), sample_size=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    classes = 10\n",
    "    values_train = y_train.reshape(-1)\n",
    "    enc_y = np.eye(classes)[values_train]\n",
    "    return enc_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def error_score(y, y_pred):\n",
    "    return 1 - accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Results\n",
    "We split our data into 80%/20% train and test. We perform 20 runs for $d = 1, ..., 7$, and report the mean test and training errors with their standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic run for part 1.1\n",
    "\n",
    "def basic_run(X_train, X_test, y_train, y_test, k, s=1):    \n",
    "    #fit model\n",
    "    mlknn = MLkNN(X_train, y_train, k, s)\n",
    "    mlknn.fit()\n",
    "    \n",
    "    #return errors\n",
    "    error_train = error_score(y_train, np.argmax(mlknn.predict(X_train), axis=1))\n",
    "    error_test = error_score(y_test, np.argmax(mlknn.predict(X_test), axis=1))\n",
    "    \n",
    "    return {'err_train': error_train, 'err_test': error_test, 'model': mlknn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]\n",
      "  5%|████▏                                                                              | 1/20 [00:05<01:40,  5.28s/it]\n",
      " 10%|████████▎                                                                          | 2/20 [00:10<01:35,  5.30s/it]\n",
      " 15%|████████████▍                                                                      | 3/20 [00:15<01:30,  5.31s/it]\n",
      " 20%|████████████████▌                                                                  | 4/20 [00:21<01:25,  5.32s/it]\n",
      " 25%|████████████████████▊                                                              | 5/20 [00:26<01:20,  5.33s/it]\n",
      " 30%|████████████████████████▉                                                          | 6/20 [00:32<01:14,  5.34s/it]\n",
      " 35%|█████████████████████████████                                                      | 7/20 [00:37<01:09,  5.35s/it]\n",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [00:42<01:04,  5.38s/it]\n",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:48<00:59,  5.37s/it]\n",
      " 50%|█████████████████████████████████████████                                         | 10/20 [00:53<00:53,  5.38s/it]\n",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:59<00:48,  5.40s/it]\n",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [01:04<00:42,  5.37s/it]\n",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [01:09<00:37,  5.36s/it]\n",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [01:15<00:32,  5.35s/it]\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [01:20<00:26,  5.38s/it]\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [01:26<00:21,  5.43s/it]\n",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [01:31<00:16,  5.43s/it]\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [01:36<00:10,  5.45s/it]\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [01:42<00:05,  5.42s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:47<00:00,  5.41s/it]"
     ]
    }
   ],
   "source": [
    "# perform basic runs\n",
    "iterations = 20\n",
    "list_ks = [1, 2, 3, 4]\n",
    "err_train = {k: [] for k in list_ks}\n",
    "err_test = {k: [] for k in list_ks}\n",
    "\n",
    "for iteration in tqdm(list(range(iterations))):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    for k in list_ks:\n",
    "        #split data\n",
    "        results = basic_run(X_train, X_test, y_train, y_test, k)\n",
    "        err_train[k].append(results['err_train'])\n",
    "        err_test[k].append(results['err_test'])\n",
    "    \n",
    "err_train_mean = {d: np.mean(errs) for d, errs in err_train.items()}\n",
    "err_test_mean = {d: np.mean(errs) for d, errs in err_test.items()}\n",
    "err_train_std = {d: np.std(errs) for d, errs in err_train.items()}\n",
    "err_test_std = {d: np.std(errs) for d, errs in err_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_mean</th>\n",
       "      <th>test_mean</th>\n",
       "      <th>train_std</th>\n",
       "      <th>test_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097063</td>\n",
       "      <td>0.14475</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.021064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115938</td>\n",
       "      <td>0.15225</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.021649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129688</td>\n",
       "      <td>0.15975</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.021359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119312</td>\n",
       "      <td>0.14525</td>\n",
       "      <td>0.011999</td>\n",
       "      <td>0.027133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_mean  test_mean  train_std  test_std\n",
       "1    0.097063    0.14475   0.005173  0.021064\n",
       "2    0.115938    0.15225   0.011151  0.021649\n",
       "3    0.129688    0.15975   0.010420  0.021359\n",
       "4    0.119312    0.14525   0.011999  0.027133"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display in dataframe\n",
    "df_err = pd.DataFrame([err_train_mean, err_test_mean,\n",
    "                       err_train_std, err_test_std], \n",
    "                       index=['train_mean', 'test_mean', 'train_std', 'test_std'], \n",
    "                       columns=list_ks).T\n",
    "df_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-validation\n",
    "\n",
    "We split our data into 80%/20% train and test. We then use 5-fold cross validation to find our best $d^*$ parameter for $d^* \\in \\{1, ..., 7\\}$. We then retrain our optimal kernelised perceptron on the full training set, and calculate training and test errors over 20 runs. We report the mean test and training errors for this perceptron, as well as its standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fold_indices(n, k=5):\n",
    "    ixs = np.array(range(n))\n",
    "    np.random.shuffle(ixs)\n",
    "    folds = np.array_split(ixs, k)\n",
    "    fold_ixs = np.zeros(n)\n",
    "    for i in range(k):\n",
    "        fold_ixs[folds[i]] = i\n",
    "    return fold_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate k folds and perform cross-validation on them, returning error per fold.\n",
    "def cross_validation_error(X, y, kernel, epochs=2, k=5):\n",
    "    fold_ixs = make_fold_indices(len(X), k=k)\n",
    "\n",
    "    cv_errs = []\n",
    "    for fold_ix in np.unique(fold_ixs):\n",
    "        X_val = X[fold_ixs == fold_ix]\n",
    "        y_val = y[fold_ixs == fold_ix]\n",
    "        X_train = X[fold_ixs != fold_ix]\n",
    "        y_train = y[fold_ixs != fold_ix]\n",
    "        \n",
    "        #fit model\n",
    "        mkp = VectorizedOneVsOneKernelPerceptron(X_train, y_train, kernel)\n",
    "        mkp.train_for_epochs(epochs=5)\n",
    "        \n",
    "        #record validation fold error\n",
    "        mkp.train_for_epochs(epochs)\n",
    "        cv_errs.append(error_score(y_val, mkp.predict_all(X_val)))\n",
    "        \n",
    "    return np.mean(cv_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross-validation runs\n",
    "\n",
    "iterations = 20\n",
    "ds = list(range(1, 8))\n",
    "errs_cv = {}\n",
    "\n",
    "d_stars = []\n",
    "errs_test = []\n",
    "confusion_matrices = []\n",
    "for iteration in tqdm(list(range(iterations))):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    \n",
    "    # perform cross validations\n",
    "    for d in ds:\n",
    "        errs_cv[d] = cross_validation_error(X_train, y_train, polynomial_kernel(d))\n",
    "        \n",
    "    # get best parameter\n",
    "    d_star = min(errs_cv, key=errs_cv.get)\n",
    "    d_stars.append(d_star)\n",
    "    \n",
    "    # get final error\n",
    "    results = basic_run(X_train, X_test, y_train, y_test, polynomial_kernel(d_star))\n",
    "    errs_test.append(results['err_test'])\n",
    "\n",
    "    \n",
    "# compute results   \n",
    "err_test_mean = np.mean(errs_test)\n",
    "d_star_mean = np.mean(d_stars)\n",
    "err_test_std = np.std(errs_test)\n",
    "d_star_std = np.std(d_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in dataframe\n",
    "df_err = pd.DataFrame([[err_test_mean, err_test_std],\n",
    "                       [d_star_mean, d_star_std]], \n",
    "                       columns=['mean', 'std'], index=['err_test', 'd_star']).T\n",
    "print(\"Answer to 2:\")\n",
    "df_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
